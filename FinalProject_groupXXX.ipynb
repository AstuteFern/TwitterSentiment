{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airline Sentiment Analysis\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Sukhman Virk\n",
    "- Athira Rajiv\n",
    "- Neil Bajaj\n",
    "- Yash Sharma\n",
    "- Lucas Fasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "In this study, we address the declining customer satisfaction in the US airline industry, which has been exacerbated by efforts to cut costs. Despite reduced prices, public sentiment reflects a deterioration in service quality, as captured in customer feedback on social media. Utilizing a dataset of approximately 14,600 tweets, we employ natural language processing (NLP) techniques, specifically Term Frequency-Inverse Document Frequency (TF-IDF), to quantitatively analyze customer sentiment. Our approach involves using an unsupervised machine learning algorithm, K-means, to categorize feedback into distinct clusters. We form clusters using negative feedback from individual airlines, allowing us to identify and categorize the main areas needing improvement. To evaluate the relevance of the feedback to actual service issues, we implemented topic modeling to uncover a the main issue of the feedback in each cluster. The effectiveness of our clustering was measured using the silhouette score. We also had a negative sentiment reason column in our table that helped us verify whether our clusters accurately clustered according to the feedback that was manually extracted from the text. Although our analysis yielded clusters with moderate silhouette scores, indicating a blend of sentiments, a comparative analysis of the clusters and manually labeled issues revealed a rough correlation. This suggests that as more data is incorporated over time, the clarity and distinction of these clusters are likely to improve, potentially leading to more targeted and effective strategies for airlines to enhance their service quality. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Social media platforms, particularly Twitter, have become a popular source for customer feedback in recent years. Analyzing customer sentiment can be a powerful tool for measuring customer satisfaction and identifying areas of improvement in industries such as the airline industry. Because Twitter is a real-time platform and is used by millions of people, it has become a valuable source for analyzing customer sentiment. NLP techniques can be used to preprocess and analyze Twitter text data to gather valuable information about customers' opinions about different products and services<a name=\"note1\"></a>[<sup>[1]</sup>](#note1).\n",
    "\n",
    "Previous studies in this field have concentrated on a number of important factors. A significant amount of research has been done on the preprocessing and analysis of Twitter text data using natural language processing (NLP) techniques like TF-IDF, word input, and emotional vocabulary<a name=\"note2\"></a>[<sup>[2]</sup>](#note2) numerical format that is appropriate for machine learning algorithms.\n",
    "\n",
    "Moreover, existing studies have focused on classifying tweets into various sentiments and have found themes or patterns within those sentiments through the use of clustering algorithms. For this, K-Means clustering, hierarchical clustering, and Gaussian mixture model (GMM) clustering are frequently employed. A particular study focused on a multitude of clustering approaches with regard to sentiment analysis, finding that the use of clustering algorithms quickly and effectively separated tweets based on their sentiment scores<a name=\"note3\"></a>[<sup>[3]</sup>](#note3).\n",
    "\n",
    "As proven in other studies, sentiment analysis in tweets can help us decipher people’s opinions, sentiments, attitudes, and emotions from merely written text. In a study conducted by Barreto, Moura, Carvalho, Paes and Plastino, they tend to address the sentiment classification of tweets, a task made challenging by the informal style of language, use of slang, and the presence of misspellings and grammatical inconsistencies<a name=\"note4\"></a>[<sup>[4]</sup>](#note4). By evaluating these words with not only traditional vector space models like Term Frequency-Inverse Document Frequency (TF-IDF), but also with advanced methods like Word2Vec, FastText, and GloVe, this study offers a more comprehensive understanding of the effectiveness of different word representation models in the context of Twitter sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "We want to enhance customer satisfaction among users of US airlines. The approach involves utilizing clustering algorithms to organize customer feedback data obtained from Twitter, addressing various airlines operating in America. The primary objective is to create clusters representing different sentiments, enabling the identification of airlines with the highest customer satisfaction and pinpointing specific areas each airline needs to focus on to enhance overall customer satisfaction.\n",
    "To address this problem, natural language processing (NLP) techniques like TF-IDF will be applied to convert textual input into a numerical format suitable for machine learning algorithms. The numeric representations derived from NLP techniques will then be utilized in the K-means clustering algorithm. This process ensures the problem's quantifiability by translating qualitative feedback into quantitative data for analysis.\n",
    "The effectiveness of the clustering process will be measured using metrics, namely the silhouette score, which assesses the cohesion and separation of clusters. Additionally, the performance of clustering algorithms will be evaluated visually using Principal Component Analysis (PCA). This allows for the observation of how well customer feedback is grouped into distinct sentiments and themes within those sentiments, providing a measurable assessment of the clustering process.\n",
    "The problem is replicable as it involves analyzing customer feedback, a continuously growing dataset. By applying sentiment analysis techniques to newer data, changes in customer sentiment and satisfaction can be monitored over time. The clustering model can be updated with the inclusion of new data, ensuring its adaptability to shifting customer expectations. This replicable nature enables airlines to stay abreast of evolving customer sentiments, facilitating timely adjustments to their services to meet customer needs effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment/data\n",
    "\n",
    "\n",
    "The dataset utilized for this analysis contains Twitter data concerning customer feedback on US airlines. With several variables including 'tweet_created', 'text', and 'airline_sentiment', the dataset encompasses a total of 14,600 observations across 15 variables, each representing a tweet detailing a user's experience with a specific airline. Critical variables such as 'tweet_created', indicating the timestamp of tweet creation, 'text', containing the tweet content, and 'airline_sentiment', denoting the sentiment expressed in the tweet (positive, negative, or neutral), are integral to the analysis.\n",
    "To prepare the data for analysis, several cleaning steps were undertaken. Initially, the 'tweet_created' variable was converted to datetime format using the pd.to_datetime() function, with separate columns created to extract date and time components. Following this, irrelevant columns such as 'tweet_location', 'retweet_count', and others deemed unnecessary were removed from the dataset. Additionally, observations with missing values in the 'text' column were dropped to ensure the integrity of the text data.\n",
    "Text cleaning procedures were then implemented using a custom function named clean_text(). This function removed Twitter handles (@username), replaced non-alphanumeric characters with spaces, converted text to lowercase, and removed single characters and multiple whitespaces for consistency. Furthermore, stopwords, common words like 'the', 'and', 'is', etc., were removed from the cleaned text using NLTK's set of English stopwords.\n",
    "The cleaned text data was finally stored in a new column labeled 'cleaned text' for further analysis. These data cleaning processes ensure the integrity and reliability of the dataset, laying a solid foundation for subsequent analysis and insights extraction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/svirk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/svirk/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_created'] = pd.to_datetime(df['tweet_created'], format='%Y-%m-%d %H:%M:%S %z')\n",
    "df['date'] = df['tweet_created'].dt.strftime('%m, %d, %Y')\n",
    "df['time'] = df['tweet_created'].dt.strftime('%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['tweet_location','retweet_count','name','airline_sentiment_gold','negativereason_gold','tweet_created','tweet_coord','negativereason_confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>text</th>\n",
       "      <th>user_timezone</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>American</th>\n",
       "      <td>2759</td>\n",
       "      <td>2759</td>\n",
       "      <td>2759</td>\n",
       "      <td>1960</td>\n",
       "      <td>2759</td>\n",
       "      <td>1791</td>\n",
       "      <td>2759</td>\n",
       "      <td>2759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delta</th>\n",
       "      <td>2222</td>\n",
       "      <td>2222</td>\n",
       "      <td>2222</td>\n",
       "      <td>955</td>\n",
       "      <td>2222</td>\n",
       "      <td>1613</td>\n",
       "      <td>2222</td>\n",
       "      <td>2222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Southwest</th>\n",
       "      <td>2420</td>\n",
       "      <td>2420</td>\n",
       "      <td>2420</td>\n",
       "      <td>1186</td>\n",
       "      <td>2420</td>\n",
       "      <td>1659</td>\n",
       "      <td>2420</td>\n",
       "      <td>2420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US Airways</th>\n",
       "      <td>2913</td>\n",
       "      <td>2913</td>\n",
       "      <td>2913</td>\n",
       "      <td>2263</td>\n",
       "      <td>2913</td>\n",
       "      <td>1851</td>\n",
       "      <td>2913</td>\n",
       "      <td>2913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United</th>\n",
       "      <td>3822</td>\n",
       "      <td>3822</td>\n",
       "      <td>3822</td>\n",
       "      <td>2633</td>\n",
       "      <td>3822</td>\n",
       "      <td>2527</td>\n",
       "      <td>3822</td>\n",
       "      <td>3822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virgin America</th>\n",
       "      <td>504</td>\n",
       "      <td>504</td>\n",
       "      <td>504</td>\n",
       "      <td>181</td>\n",
       "      <td>504</td>\n",
       "      <td>379</td>\n",
       "      <td>504</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id  airline_sentiment  airline_sentiment_confidence  \\\n",
       "airline                                                                     \n",
       "American            2759               2759                          2759   \n",
       "Delta               2222               2222                          2222   \n",
       "Southwest           2420               2420                          2420   \n",
       "US Airways          2913               2913                          2913   \n",
       "United              3822               3822                          3822   \n",
       "Virgin America       504                504                           504   \n",
       "\n",
       "                negativereason  text  user_timezone  date  time  \n",
       "airline                                                          \n",
       "American                  1960  2759           1791  2759  2759  \n",
       "Delta                      955  2222           1613  2222  2222  \n",
       "Southwest                 1186  2420           1659  2420  2420  \n",
       "US Airways                2263  2913           1851  2913  2913  \n",
       "United                    2633  3822           2527  3822  3822  \n",
       "Virgin America             181   504            379   504   504  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=['airline']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'@\\w+', '', str(text))\n",
    "    text = re.sub(r'\\W', ' ', str(text))\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+[a-z]\\s+', ' ',text)\n",
    "    text = re.sub(r'^[a-z]\\s+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "df['cleaned text'] = df['cleaned text'].apply(lambda x: ' '.join(term for term in x.split() if term not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentiment_to_numeric(sentiment):\n",
    "    sentiment_map = {\"positive\": 1, \"negative\": -1, \"neutral\": 0}\n",
    "    return sentiment_map.get(sentiment.lower(), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_153/4221351807.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'our_airline_sentiment_val'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_sentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m'negative'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4755\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4756\u001b[0m         \"\"\"\n\u001b[0;32m-> 4757\u001b[0;31m         return SeriesApply(\n\u001b[0m\u001b[1;32m   4758\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4759\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1290\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m         return lib.map_infer_mask(\n",
      "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_153/1875640920.py\u001b[0m in \u001b[0;36mget_sentiment\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compound'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/nltk/sentiment/vader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lexicon_file)\u001b[0m\n\u001b[1;32m    339\u001b[0m     ):\n\u001b[1;32m    340\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicon_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexicon_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_lex_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVaderConstants\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/nltk/sentiment/vader.py\u001b[0m in \u001b[0;36mmake_lex_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mlex_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicon_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m             \u001b[0mlex_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlex_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df['our_airline_sentiment_val'] = df['cleaned text'].apply(get_sentiment)\n",
    "def convert_val(val):\n",
    "    if val < 0:\n",
    "        return 'negative'\n",
    "    elif val > 0:\n",
    "        return 'positive'\n",
    "    else :\n",
    "        return 'neutral'\n",
    "df['our_airline_sentiment'] = df['our_airline_sentiment_val'].apply(convert_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"airline_sentiment_numeric\"] = df[\"airline_sentiment\"].apply(convert_sentiment_to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "To enhance customer satisfaction for US Airlines users by analyzing Twitter feedback, our proposed solution utilizes Natural Language Processing and Machine Learning techniques to cluster customer feedback into distinct sentiment groups. Initially, the data undergoes some basic text cleanup, where unnecessary text is removed from non-alphanumeric characters and any columns we will not be using will be dropped. Shortly after this, the TF-IDF method is applied to convert tweets into a numeric vector form, that we can use in our clustering algorithms. For clustering, we use algorithms such as K-Means or Gaussian Mixture Models. Our appropriate choice will depend on the accruacy of our clustering along with comparing it to the manually labelled data to see whether we have any correlation. Upon gathering our cluster results we will apply topic modeling to observe if there is any relation to the most common topics in the cluster and the actual reasons provided through the dataset.The goal is to enable the model to accurately identify the sentiment of new data and cluster it accordingly.\n",
    "\n",
    "Once we have finished clustering the data a silhouette score will evaluate the quality of our cluster. This ensures adaptability to new data and can be tested for future data sets. It also is an easy metric for people viewing the data to understand. We will also leverage the capabilities of Topic modelling in our clusters and try to see if our clusters accurately model the reason behind the negative tweets in the cluster. This approach aims to provide actionable insights into customer sentiments, helping airlines on areas needing improvement. Through this entire process airlines will be able to identify areas of improvement provided with accurate data in an easy to understand format that can be scaled to ever increasing customer reviews.\n",
    "Topic modeling the clusters will also let us see if similar negative feedback tweets fall into the same cluster, this will help us find out if our model is scalable and when more data is input into it, it will feed the negative feedback into the corresponding clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Since we will be using clustering as our main algorithm to find solutions for our proposed question, the Silhouette score will be the main evaluation metric for our project. The Silhouette coefficient is a metric that measures how well each data point fits into its cluster. It provides information about both the cohesion and the separation of the data point. The value of the silhouette coefﬁcient is between -1 and 1. A score of 1 denotes the best, meaning that the data point i is very compact within the cluster to which it belongs and is far away from the other clusters where it does not belong. This evaluation metric will help us determine how good our clustering is and if we are ready to move forward with the next steps of our project [topic modeling] to find out the main theme, positive points, and negative points for each cluster and data point. \n",
    "However, the silhoutte score isn't the only measure we're using. Since we have a column in our dataset which corresponds to the reason of negative feedback for the airline that is manually inputted, we can use that as a measure as to how accurate our clusters are. Even if our clusters have a low sillhoutte score and are not very distinct from each other, we can check whether our clusters, via topic modelling, correlate to the manual labells provided to us. If they do, we can claim that as the dataset increases in size, we wouldn't need to manually label the data, and the data would fall into the cluster with feedback of similar reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['airline_sentiment_numeric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned text'], df['airline_sentiment_numeric'], test_size=0.2, random_state=42)\n",
    "model = make_pipeline(TfidfVectorizer(), LogisticRegression())\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy = model.score(X_test, y_test)\n",
    "print(f'Model accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_sentiment_linReg'] = model.predict(df['cleaned text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(TfidfVectorizer(), SVC())\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracy = model.score(X_test, y_test)\n",
    "print(f'Model accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_sentiment_SVC'] = model.predict(df['cleaned text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['airline'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# United Airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_united = df[df['airline'] == 'United']\n",
    "df_united_neg = df_united[df_united['airline_sentiment'] == 'negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a TF-IDF vectorizer to convert text data into numerical vectors\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "\n",
    "# Apply TF-IDF vectorization to the cleaned text data of negative tweets from United Airlines\n",
    "tfidf_matrix_united_neg = tfidf_vectorizer.fit_transform(df_united_neg['cleaned text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Determine the optimal number of components for PCA using the elbow method\n",
    "n_components = min(tfidf_matrix_united_neg.shape)\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_transformed_data = pca.fit_transform(tfidf_matrix_united_neg.toarray())  # Convert sparse matrix to dense\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "eigenvalues = pca.explained_variance_\n",
    "plt.plot(sorted(eigenvalues, reverse=True))\n",
    "plt.xlabel('Index of Eigenvalues')\n",
    "plt.ylabel('Eigenvalues (in descending order)')\n",
    "plt.title('Elbow Method for PCA Components Determination')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 300 #number of components using elbow method\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_transformed_data = pca.fit_transform(tfidf_matrix_united_neg.toarray())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pca_transformed_data[:, 0], pca_transformed_data[:, 1], s=10)\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_negative_reasons = df_united_neg['negativereason'].nunique()\n",
    "unique_negative_reasons\n",
    "\n",
    "# Apply K-means clustering using the determined number of clusters based on unique negative reasons\n",
    "kmeans = KMeans(n_clusters=unique_negative_reasons, random_state=42)\n",
    "kmeans.fit(pca_transformed_data)\n",
    "\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "\n",
    "df_united_neg['cluster'] = cluster_labels\n",
    "\n",
    "silhouette_avg = silhouette_score(pca_transformed_data, cluster_labels)\n",
    "print(f\"Average Silhouette Score: {silhouette_avg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform topic modeling (Latent Dirichlet Allocation) for each cluster to extract main topics\n",
    "n_topics_per_cluster = 1  # Assuming one main topic per cluster\n",
    "lda_models = {}\n",
    "for i in range(unique_negative_reasons):\n",
    "    cluster_texts = tfidf_matrix_united_neg[df_united_neg['cluster'] == i]\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics_per_cluster, random_state=0)\n",
    "    lda.fit(cluster_texts)\n",
    "    lda_models[i] = lda\n",
    "    print(f\"Cluster {i} (corresponding to negative reasons):\")\n",
    "    tf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([tf_feature_names[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "        print(message)\n",
    "    print(\"Actual negative reasons in this cluster:\")\n",
    "    print(df_united_neg[df_united_neg['cluster'] == i]['negativereason'].value_counts())\n",
    "    print(\"\\n\" + \"-\" * 40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# American Airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_am = df[df['airline'] == 'American']\n",
    "df_am_neg = df_am[df_am['airline_sentiment'] == 'negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf_matrix_am_neg = tfidf_vectorizer.fit_transform(df_am_neg['cleaned text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = min(tfidf_matrix_am_neg.shape)\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_transformed_data = pca.fit_transform(tfidf_matrix_am_neg.toarray()) \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "eigenvalues = pca.explained_variance_\n",
    "plt.plot(sorted(eigenvalues, reverse=True))\n",
    "plt.xlabel('Index of Eigenvalues')\n",
    "plt.ylabel('Eigenvalues (in descending order)')\n",
    "plt.title('Elbow Method for PCA Components Determination for American Airlines')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 125 #number of components using elbow method\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_transformed_data = pca.fit_transform(tfidf_matrix_am_neg.toarray())\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pca_transformed_data[:, 0], pca_transformed_data[:, 1], s=10)\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_negative_reasons = df_am_neg['negativereason'].nunique()\n",
    "unique_negative_reasons\n",
    "\n",
    "kmeans = KMeans(n_clusters=unique_negative_reasons, random_state=42)\n",
    "kmeans.fit(pca_transformed_data)\n",
    "\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "\n",
    "df_am_neg['cluster'] = cluster_labels\n",
    "\n",
    "silhouette_avg = silhouette_score(pca_transformed_data, cluster_labels)\n",
    "print(f\"Average Silhouette Score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_am_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics_per_cluster = 1  # Assuming one main topic per cluster\n",
    "lda_models = {}\n",
    "for i in range(unique_negative_reasons):\n",
    "    cluster_texts = tfidf_matrix_am_neg[df_am_neg['cluster'] == i]\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics_per_cluster, random_state=0)\n",
    "    lda.fit(cluster_texts)\n",
    "    lda_models[i] = lda\n",
    "    print(f\"Cluster {i} (corresponding to negative reasons):\")\n",
    "    tf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([tf_feature_names[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "        print(message)\n",
    "    print(\"Actual negative reasons in this cluster:\")\n",
    "    print(df_am_neg[df_am_neg['cluster'] == i]['negativereason'].value_counts())\n",
    "    print(\"\\n\" + \"-\" * 40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delta = df[df['airline'] == 'Delta']\n",
    "df_delta_neg = df_delta[df_delta['airline_sentiment'] == 'negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf_matrix_delta_neg = tfidf_vectorizer.fit_transform(df_delta_neg['cleaned text'])\n",
    "\n",
    "n_components = min(tfidf_matrix_delta_neg.shape)\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_transformed_data = pca.fit_transform(tfidf_matrix_delta_neg.toarray()) \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "eigenvalues = pca.explained_variance_\n",
    "plt.plot(sorted(eigenvalues, reverse=True))\n",
    "plt.xlabel('Index of Eigenvalues')\n",
    "plt.ylabel('Eigenvalues (in descending order)')\n",
    "plt.title('Elbow Method for PCA Components Determination for American Airlines')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 90 #elbow method: number of components\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_transformed_data = pca.fit_transform(tfidf_matrix_delta_neg.toarray())\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pca_transformed_data[:, 0], pca_transformed_data[:, 1], s=10)\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_negative_reasons = df_delta_neg['negativereason'].nunique()\n",
    "unique_negative_reasons\n",
    "\n",
    "kmeans = KMeans(n_clusters=unique_negative_reasons, random_state=42)\n",
    "kmeans.fit(pca_transformed_data)\n",
    "\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "\n",
    "df_delta_neg['cluster'] = cluster_labels\n",
    "\n",
    "silhouette_avg = silhouette_score(pca_transformed_data, cluster_labels)\n",
    "print(f\"Average Silhouette Score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics_per_cluster = 1  # Assuming one main topic per cluster\n",
    "lda_models = {}\n",
    "for i in range(unique_negative_reasons):\n",
    "    cluster_texts = tfidf_matrix_delta_neg[df_delta_neg['cluster'] == i]\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics_per_cluster, random_state=0)\n",
    "    lda.fit(cluster_texts)\n",
    "    lda_models[i] = lda\n",
    "    print(f\"Cluster {i} (corresponding to negative reasons):\")\n",
    "    tf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([tf_feature_names[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "        print(message)\n",
    "    print(\"Actual negative reasons in this cluster:\")\n",
    "    print(df_delta_neg[df_delta_neg['cluster'] == i]['negativereason'].value_counts())\n",
    "    print(\"\\n\" + \"-\" * 40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virgin America"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_virg = df[df['airline'] == 'Virgin America']\n",
    "df_virg_neg = df_virg[df_virg['airline_sentiment'] == 'negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf_matrix_virg_neg = tfidf_vectorizer.fit_transform(df_virg_neg['cleaned text'])\n",
    "\n",
    "n_components = min(tfidf_matrix_virg_neg.shape)\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_transformed_data = pca.fit_transform(tfidf_matrix_virg_neg.toarray())  \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "eigenvalues = pca.explained_variance_\n",
    "plt.plot(sorted(eigenvalues, reverse=True))\n",
    "plt.xlabel('Index of Eigenvalues')\n",
    "plt.ylabel('Eigenvalues (in descending order)')\n",
    "plt.title('Elbow Method for PCA Components Determination for American Airlines')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 25 #elbow method: number of components\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_transformed_data = pca.fit_transform(tfidf_matrix_virg_neg.toarray())\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pca_transformed_data[:, 0], pca_transformed_data[:, 1], s=10)\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_negative_reasons = df_virg_neg['negativereason'].nunique()\n",
    "unique_negative_reasons\n",
    "\n",
    "kmeans = KMeans(n_clusters=unique_negative_reasons, random_state=80)\n",
    "kmeans.fit(pca_transformed_data)\n",
    "\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "\n",
    "df_virg_neg['cluster'] = cluster_labels\n",
    "\n",
    "silhouette_avg = silhouette_score(pca_transformed_data, cluster_labels)\n",
    "print(f\"Average Silhouette Score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics_per_cluster = 1  # Assuming one main topic per cluster\n",
    "lda_models = {}\n",
    "for i in range(unique_negative_reasons):\n",
    "    cluster_texts = tfidf_matrix_virg_neg[df_virg_neg['cluster'] == i]\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics_per_cluster, random_state=0)\n",
    "    lda.fit(cluster_texts)\n",
    "    lda_models[i] = lda\n",
    "    print(f\"Cluster {i} (corresponding to negative reasons):\")\n",
    "    tf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([tf_feature_names[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "        print(message)\n",
    "    print(\"Actual negative reasons in this cluster:\")\n",
    "    print(df_virg_neg[df_virg_neg['cluster'] == i]['negativereason'].value_counts())\n",
    "    print(\"\\n\" + \"-\" * 40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Southwest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sw = df[df['airline'] == 'Southwest']\n",
    "df_sw_neg = df_sw[df_sw['airline_sentiment'] == 'negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf_matrix_sw_neg = tfidf_vectorizer.fit_transform(df_sw_neg['cleaned text'])\n",
    "\n",
    "n_components = min(tfidf_matrix_sw_neg.shape)\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_transformed_data = pca.fit_transform(tfidf_matrix_sw_neg.toarray())  \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "eigenvalues = pca.explained_variance_\n",
    "plt.plot(sorted(eigenvalues, reverse=True))\n",
    "plt.xlabel('Index of Eigenvalues')\n",
    "plt.ylabel('Eigenvalues (in descending order)')\n",
    "plt.title('Elbow Method for PCA Components Determination for American Airlines')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 90 #elbow metehod: number of components\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_transformed_data = pca.fit_transform(tfidf_matrix_sw_neg.toarray())\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pca_transformed_data[:, 0], pca_transformed_data[:, 1], s=10)\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_negative_reasons = df_sw_neg['negativereason'].nunique()\n",
    "unique_negative_reasons\n",
    "\n",
    "kmeans = KMeans(n_clusters=unique_negative_reasons, random_state=80)\n",
    "kmeans.fit(pca_transformed_data)\n",
    "\n",
    "# Cluster labels for each point\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "\n",
    "df_sw_neg['cluster'] = cluster_labels\n",
    "\n",
    "#Analyze the Clustering Results\n",
    "silhouette_avg = silhouette_score(pca_transformed_data, cluster_labels)\n",
    "print(f\"Average Silhouette Score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics_per_cluster = 1  # Assuming one main topic per cluster\n",
    "lda_models = {}\n",
    "for i in range(unique_negative_reasons):\n",
    "    cluster_texts = tfidf_matrix_sw_neg[df_sw_neg['cluster'] == i]\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics_per_cluster, random_state=0)\n",
    "    lda.fit(cluster_texts)\n",
    "    lda_models[i] = lda\n",
    "    print(f\"Cluster {i} (corresponding to negative reasons):\")\n",
    "    tf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([tf_feature_names[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "        print(message)\n",
    "    print(\"Actual negative reasons in this cluster:\")\n",
    "    print(df_sw_neg[df_sw_neg['cluster'] == i]['negativereason'].value_counts())\n",
    "    print(\"\\n\" + \"-\" * 40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Airways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us = df[df['airline'] == 'US Airways']\n",
    "df_us_neg = df_us[df_us['airline_sentiment'] == 'negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf_matrix_us_neg = tfidf_vectorizer.fit_transform(df_us_neg['cleaned text'])\n",
    "\n",
    "n_components = min(tfidf_matrix_us_neg.shape)\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_transformed_data = pca.fit_transform(tfidf_matrix_us_neg.toarray())  \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "eigenvalues = pca.explained_variance_\n",
    "plt.plot(sorted(eigenvalues, reverse=True))\n",
    "plt.xlabel('Index of Eigenvalues')\n",
    "plt.ylabel('Eigenvalues (in descending order)')\n",
    "plt.title('Elbow Method for PCA Components Determination for American Airlines')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 125 #elbow method: number of components\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_transformed_data = pca.fit_transform(tfidf_matrix_us_neg.toarray())\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pca_transformed_data[:, 0], pca_transformed_data[:, 1], s=10)\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_negative_reasons = df_us_neg['negativereason'].nunique()\n",
    "unique_negative_reasons\n",
    "\n",
    "kmeans = KMeans(n_clusters=unique_negative_reasons, random_state=80)\n",
    "kmeans.fit(pca_transformed_data)\n",
    "\n",
    "# Cluster labels for each point\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "\n",
    "df_us_neg['cluster'] = cluster_labels\n",
    "\n",
    "#Analyze the Clustering Results\n",
    "silhouette_avg = silhouette_score(pca_transformed_data, cluster_labels)\n",
    "print(f\"Average Silhouette Score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics_per_cluster = 1  # Assuming one main topic per cluster\n",
    "lda_models = {}\n",
    "for i in range(unique_negative_reasons):\n",
    "    cluster_texts = tfidf_matrix_us_neg[df_us_neg['cluster'] == i]\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics_per_cluster, random_state=0)\n",
    "    lda.fit(cluster_texts)\n",
    "    lda_models[i] = lda\n",
    "    print(f\"Cluster {i} (corresponding to negative reasons):\")\n",
    "    tf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([tf_feature_names[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "        print(message)\n",
    "    print(\"Actual negative reasons in this cluster:\")\n",
    "    print(df_us_neg[df_us_neg['cluster'] == i]['negativereason'].value_counts())\n",
    "    print(\"\\n\" + \"-\" * 40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "Our analysis suggests a correlation between manually labeled negative reasons and the issues identified within clusters formed by our unsupervised approach. This shows that we could potentially use the customer feedback data to automate the categorization of airline issues and identify areas for improvement. The topics extracted through Latent Dirichlet Allocation (LDA) within each cluster align somewhat with the most common negative reasons filled into the dataset manually, supporting the idea that our clustering captures meaningful patterns in the data. Despite the silhouette score for K-means clustering not being high, suggesting some overlap between clusters, the observed correlation between our clusters and manually labeled issues suggests that with more data, the clusters could become more distinct and informative. Additionally, high variability in the types of negative reasons across clusters highlights that different clusters capture different aspects of customer dissatisfaction which is not the case as we see here. We also can know that overlapping issues across clusters could imply potential ambiguity among certain negative reasons but since it rarely occurs in our analysis, it can be ignored for now.  \n",
    "\n",
    "### Limitations\n",
    "\n",
    "One primary limitation of our data analysis was the low silhouette scores obtained for K-means clustering. Attempting alternative clustering algorithms such as GMM and DBSCAN only yielded even lower silhouette scores, indicating poor cluster quality and significant overlap between data points within clusters. We also tried alternative methods of dimensionality reduction, including PCA and tSNE, to explore different representations of the tweet data and potentially improve clustering performance, but these methods did not significantly improve the silhouette scores or cluster quality. This limitation suggests challenges in effectively partitioning the data into distinct and homogeneous groups, potentially undermining the reliability and interpretability of our clustering results. Another limitation of our work is the potential need for more data to improve the effectiveness of our clustering approach. As we acquire more data, we hope that our model will form more distinct clusters, increasing the reliability and accuracy of our results over time. Furthermore, our analysis focuses solely on tweets and may not capture the entirety of customer sentiment -- this may not fully represent the diverse range of customer feedback channels airlines receive. Other sources, such as customer surveys or reviews on different platforms, could provide additional insights or perspectives that are not captured in tweets alone.\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "An ethical concern in our project revolves around ensuring the privacy of individuals whose tweets are used for analysis. Despite anonymizing the data, there is still a risk of inadvertently identifying individuals through their tweets, especially if they contain unique personal information. To address this concern, we adhere to strict data anonymization practices and ensure compliance with data privacy regulations. Additionally, ongoing monitoring and evaluation of the automated categorization system can help identify and mitigate any potential biases or ethical issues that may arise. We also removed the tweet id's from the dataset that could lead to identifyign the people who made the tweet, increasing the privay.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "In conclusion, our study leverages natural language processing and machine learning to analyze tweets about US airlines, successfully categorizing customer feedback into clusters that highlight areas needing improvement. We trained a sentiment analysis model that could detect the sentiment of new tweets added to the model. Despite low silhouette scores on our clustering algorithms, indicating mixed sentiments, a comparative analysis suggests a correlation between clusters and specific service issues. This approach underscores the potential of using social media feedback for strategic insights, promising more targeted and effective improvements in airline service quality as data volume and analysis techniques evolve. We tried our best to take a project that can be genuinely simulated in the real world. In a world where customer feedback is extremely important, appropriately categorizing sentiment is not just helpful for airlines but across many industries as well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"note1\"></a>1.[^](#note1):  Gohil, S., Vuik, S., & Darzi, A. (2018). Sentiment Analysis of Health Care Tweets: Review of the Methods Used. JMIR Public Health. https://pubmed.ncbi.nlm.nih.gov/29685871/\n",
    "\n",
    "<a name=\"note2\"></a>2.[^](#note2):  Hasan, M. R., Maliha, M., & Arifuzzaman, M. (2019). Sentiment Analysis with NLP on Twitter Data. 2019 International Conference on Computer, Communication, Chemical, Materials and Electronic Engineering. https://ieeexplore.ieee.org/abstract/document/9036670\n",
    "\n",
    "<a name=\"note3\"></a>3.[^](#note3):  Ahuja, S., & Dubey, G. (2017). Clustering and sentiment analysis on Twitter data. 2017 2nd International Conference on Telecommunication and Networks (TEL-NET). https://ieeexplore.ieee.org/abstract/document/8343568\n",
    "\n",
    "<a name=\"note4\"></a>4.[^](#note4): Barreto, S. et al. (2023). Sentiment analysis in tweets: an assessment study from classical to modern word representation models. Data Mining and Knowledge Discovery, 37, 318–380. https://link.springer.com/article/10.1007/s10618-022-00853-0<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
